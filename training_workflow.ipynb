{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d24da363-6374-4e34-9ae0-5aeb8b978271",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets.utils.logging import set_verbosity_error\n",
    "set_verbosity_error()\n",
    "\n",
    "import random\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "DEVICE = (\n",
    "    \"cuda:0\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {DEVICE} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c2d53a-6732-4748-b87a-bb5b06407a16",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94370238-f254-411e-b5bc-326c26e9e025",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_prior_distribution(raw_train_dataset):\n",
    "    \"\"\"\n",
    "    Calculate the prior distribution of labels in the training dataset.\n",
    "\n",
    "    This function calculates the proportion of each label in the training dataset, \n",
    "    which is used as the prior distribution for Dirichlet sampling.\n",
    "\n",
    "    Args:\n",
    "        raw_train_dataset (datasets.arrow_dataset.Dataset): The training dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - prior_distribution (numpy.ndarray): An array of label proportions.\n",
    "            - num_labels (int): The number of unique labels in the dataset.\n",
    "    \"\"\"\n",
    "        \n",
    "    # Calculate the number of samples for each label\n",
    "    label_count_dict = dict(raw_train_dataset.to_pandas()['label'].value_counts())\n",
    "    # Num. of samples\n",
    "    n = len(raw_train_dataset) \n",
    "    # (label, count) list, sorted with label increasing\n",
    "    label_count_tuple = sorted(label_count_dict.items(), key=lambda x: x[0])\n",
    "    # prior distribution\n",
    "    prior_distribution = np.array([c / n for _, c in label_count_tuple])\n",
    "    \n",
    "    # number of labels\n",
    "    num_labels = len(label_count_dict)\n",
    "    \n",
    "    return prior_distribution, num_labels\n",
    "\n",
    "\n",
    "def sample_to_client(client_sample_counts, sample_label):\n",
    "    \"\"\"\n",
    "    Assign a sample to a client based on the sample's label and the remaining counts for each client.\n",
    "\n",
    "    This function assigns a sample to the client that still needs more samples of the given label.\n",
    "    It decrements the count for that label for the chosen client. If no client has a remaining count\n",
    "    of at least 1 for the label, the function assigns the sample to the client with the largest leftover count\n",
    "    for that label.\n",
    "\n",
    "    Args:\n",
    "        client_sample_counts (numpy.ndarray): A 2D array where each row corresponds to a client and each column corresponds\n",
    "            to the count (float) of samples needed for each label. The element at (i, j) represents the decimal number of \n",
    "            samples of label j that client i still needs.\n",
    "        sample_label (int): The label of the sample to be assigned to a client.\n",
    "        \n",
    "    Returns:\n",
    "        int: The index of the client to which the sample has been assigned.\n",
    "    \"\"\"\n",
    "    num_clients = len(client_sample_counts)\n",
    "    client_indices = np.random.permutation(num_clients)\n",
    "    \n",
    "    for client_idx in client_indices:\n",
    "        client_sample_count = client_sample_counts[client_idx]\n",
    "        \n",
    "        if client_sample_count[sample_label] >= 1:\n",
    "            client_sample_count[sample_label] -= 1\n",
    "            \n",
    "            return client_idx\n",
    "    \n",
    "    # if all client data counts are less than 1, then assign the sample to the largest leftover\n",
    "    client_idx = np.argmax(client_sample_counts[:, sample_label])\n",
    "    client_sample_counts[client_idx][sample_label] -= 1\n",
    "    \n",
    "    return client_idx\n",
    "\n",
    "\n",
    "def federated_dirichlet_datasets(raw_train_dataset, prior_distribution, num_clients, alpha):\n",
    "    \"\"\"\n",
    "    Create federated datasets using Dirichlet-distributed label partitions.\n",
    "\n",
    "    This function partitions the training dataset into multiple subsets, each corresponding \n",
    "    to a client. The label distribution for each client is drawn from a Dirichlet distribution \n",
    "    parameterized by the given prior distribution and concentration parameter alpha.\n",
    "    The function also aims to keep the datasets as equal in size as possible.\n",
    "\n",
    "    Args:\n",
    "        raw_train_dataset (datasets.arrow_dataset.Dataset): The training dataset.\n",
    "        prior_distribution (numpy.ndarray): An array representing the prior distribution of labels.\n",
    "        num_clients (int): The number of clients.\n",
    "        alpha (float): The concentration parameter for the Dirichlet distribution.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of `datasets.arrow_dataset.Dataset` objects, each representing a client's dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Num. of samples\n",
    "    n = len(raw_train_dataset)\n",
    "    \n",
    "    client_num_samples = n / num_clients\n",
    "    \n",
    "    # Draw label distributions for each client from Dirichlet distribution\n",
    "    # Each i-th row represents the distribution of labels for the i-th client\n",
    "    client_distributions = np.random.dirichlet(alpha * prior_distribution, num_clients)\n",
    "    \n",
    "    # Initialize client data indices\n",
    "    client_sample_indices = [[] for i in range(num_clients)]\n",
    "    \n",
    "    # Calculate the number of samples per label each client should have\n",
    "    # client_sample_counts[i] is an array of `num_label` elements\n",
    "    # -- the counts for each label for the i-th client.\n",
    "    client_sample_counts = np.array([\n",
    "        client_distributions[client_idx, :] * client_num_samples\n",
    "        for client_idx in range(num_clients)\n",
    "    ])\n",
    "    \n",
    "    for sample_idx, sample in enumerate(raw_train_dataset):\n",
    "        client_idx = sample_to_client(client_sample_counts, sample['label'])\n",
    "\n",
    "        client_sample_indices[client_idx].append(sample_idx)\n",
    "        \n",
    "    # Create a Dataset for each client\n",
    "    client_datasets = []\n",
    "    for client_indices in client_sample_indices:\n",
    "        client_dataset = raw_train_dataset.select(client_indices)\n",
    "        client_datasets.append(client_dataset)\n",
    "        \n",
    "    return client_datasets\n",
    "\n",
    "\n",
    "def tokenize_client_datasets(client_datasets, tokenize_fn):\n",
    "    \"\"\"\n",
    "    Tokenize and preprocess a list of client datasets.\n",
    "\n",
    "    This function tokenizes and preprocesses each dataset in the provided list of client datasets.\n",
    "    It applies the specified tokenization function, renames the \"label\" column to \"labels\",\n",
    "    removes unnecessary columns, and sets the format to PyTorch tensors.\n",
    "\n",
    "    Args:\n",
    "        client_datasets (list of datasets.arrow_dataset.Dataset): A list of client datasets to be tokenized and preprocessed.\n",
    "        tokenization_fn (function): A function that takes an example and returns its tokenized form.\n",
    "\n",
    "    Returns:\n",
    "        list of datasets.arrow_dataset.Dataset: A list of tokenized and preprocessed client datasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the expected columns\n",
    "    expected_columns = ['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n",
    "    \n",
    "    tok_client_datasets = []\n",
    "\n",
    "    for client_dataset in client_datasets:\n",
    "        tok_client_dataset = client_dataset.map(tokenize_fn, batched=True)\n",
    "\n",
    "        tok_client_dataset = tok_client_dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "        # Identify columns to remove\n",
    "        columns_to_remove = [\n",
    "            column for column in tok_client_dataset.column_names \n",
    "            if column not in expected_columns\n",
    "        ]\n",
    "\n",
    "        # Remove unnecessary columns\n",
    "        tok_client_dataset = tok_client_dataset.remove_columns(columns_to_remove)\n",
    "\n",
    "        # Set the format to PyTorch tensors\n",
    "        tok_client_dataset.set_format(\"torch\")\n",
    "\n",
    "        # Add the processed dataset to the list\n",
    "        tok_client_datasets.append(tok_client_dataset)\n",
    "        \n",
    "    return tok_client_datasets\n",
    "\n",
    "\n",
    "def preprocess_test_dataset(raw_test_dataset, tokenize_fn, data_collator, batch_size):\n",
    "    \"\"\"\n",
    "    Preprocess and tokenize the test dataset, then create a DataLoader for it.\n",
    "\n",
    "    This function tokenizes and preprocesses the provided test dataset using the specified\n",
    "    tokenization function. It renames the \"label\" column to \"labels\", removes unnecessary columns,\n",
    "    sets the format to PyTorch tensors, and then creates a DataLoader for the test dataset.\n",
    "\n",
    "    Args:\n",
    "        raw_test_dataset (datasets.arrow_dataset.Dataset): The raw test dataset to be tokenized and preprocessed.\n",
    "        tokenize_fn (function): A function that takes an example and returns its tokenized form.\n",
    "        data_collator (transformers.DataCollator): A data collator to be used for padding and batching.\n",
    "        batch_size (int): The batch size to be used by the DataLoader.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: A DataLoader for the tokenized and preprocessed test dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the expected columns\n",
    "    expected_columns = ['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n",
    "    \n",
    "    tok_test_dataset = raw_test_dataset.map(tokenize_fn, batched=True)\n",
    "    \n",
    "    tok_test_dataset = tok_test_dataset.rename_column(\"label\", \"labels\")\n",
    "    \n",
    "    # Identify columns to remove\n",
    "    columns_to_remove = [\n",
    "        column for column in tok_test_dataset.column_names \n",
    "        if column not in expected_columns\n",
    "    ]\n",
    "    \n",
    "    # Remove unnecessary columns\n",
    "    tok_test_dataset = tok_test_dataset.remove_columns(columns_to_remove)\n",
    "    \n",
    "    # Set the format to PyTorch tensors\n",
    "    tok_test_dataset.set_format(\"torch\")\n",
    "    \n",
    "    # Create a DataLoader for the test dataset\n",
    "    test_ds = DataLoader(\n",
    "        tok_test_dataset, batch_size=batch_size, collate_fn=data_collator\n",
    "    )\n",
    "        \n",
    "    return test_ds\n",
    "\n",
    "\n",
    "def create_client_dataloaders(tok_client_datasets, batch_size, collate_fn):\n",
    "    \"\"\"\n",
    "    Create data loaders for a list of tokenized client datasets.\n",
    "\n",
    "    This function takes a list of tokenized client datasets and creates a DataLoader for each dataset.\n",
    "    The resulting data loaders are stored in a list and returned.\n",
    "\n",
    "    Args:\n",
    "        tok_client_datasets (list of datasets.arrow_dataset.Dataset): A list of tokenized client datasets.\n",
    "        batch_size (int): The batch size to be used by the data loaders.\n",
    "        collate_fn (function): A collate function to be used by the data loaders.\n",
    "\n",
    "    Returns:\n",
    "        list of DataLoader: A list of data loaders, each corresponding to a tokenized client dataset.\n",
    "    \"\"\"\n",
    "    client_dataloaders = []\n",
    "\n",
    "    for tok_client_dataset in tok_client_datasets:\n",
    "        client_dataloader = DataLoader(\n",
    "            tok_client_dataset, shuffle=True, batch_size=batch_size, collate_fn=collate_fn\n",
    "        )\n",
    "        \n",
    "        client_dataloaders.append(client_dataloader)\n",
    "        \n",
    "    return client_dataloaders\n",
    "\n",
    "\n",
    "def tokenize_function(ds_path, ds_name, tokenizer):\n",
    "    \"\"\"\n",
    "    Return a tokenization function based on the dataset path and name.\n",
    "\n",
    "    This function returns the appropriate tokenization function for the specified dataset.\n",
    "    Currently, it supports the GLUE MRPC dataset.\n",
    "\n",
    "    Args:\n",
    "        ds_path (str): The path or identifier of the dataset.\n",
    "        ds_name (str): The name of the dataset.\n",
    "        tokenizer (AutoTokenizer): The tokenizer to be used.\n",
    "\n",
    "    Returns:\n",
    "        function: A tokenization function.\n",
    "    \"\"\"\n",
    "    \n",
    "    if ds_path == \"glue\" and ds_name == \"mrpc\":\n",
    "        return lambda example: tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def test_dataset_split(raw_datasets, ds_path):\n",
    "    \"\"\"\n",
    "    Split the raw dataset into the appropriate test set.\n",
    "\n",
    "    This function extracts the test set from the provided raw dataset based on the dataset path.\n",
    "    For the GLUE dataset, it returns the validation split as the test set following prior work.\n",
    "\n",
    "    Args:\n",
    "        raw_datasets (datasets.DatasetDict): The raw dataset containing multiple splits.\n",
    "        ds_path (str): The path or identifier of the dataset.\n",
    "\n",
    "    Returns:\n",
    "        datasets.Dataset: The extracted test dataset split.\n",
    "    \"\"\"\n",
    "    \n",
    "    if ds_path == 'glue':\n",
    "        return raw_datasets['validation']\n",
    "    \n",
    "\n",
    "def prepare_federated_datasets(ds_path, ds_name, checkpoint, num_clients, alpha, batch_size):\n",
    "    \"\"\"\n",
    "    Prepare federated datasets and create corresponding DataLoaders.\n",
    "\n",
    "    This function handles the entire process of loading the raw dataset, partitioning it into\n",
    "    training federated datasets using Dirichlet distribution, tokenizing the datasets, and creating\n",
    "    DataLoaders for each client's dataset. It also tokenizes, and creates a dataloader for the test dataset.\n",
    "\n",
    "    Args:\n",
    "        ds_path (str): The path or identifier of the dataset.\n",
    "        ds_name (str): The name of the dataset.\n",
    "        checkpoint (str): The checkpoint identifier for the tokenizer.\n",
    "        num_clients (int): The number of clients.\n",
    "        alpha (float): The concentration parameter for the Dirichlet distribution.\n",
    "        batch_size (int): The batch size to be used by the DataLoaders.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - FederatedDataset: An object containing client DataLoaders for federated learning.\n",
    "            - DataLoader: A DataLoader for the tokenized and preprocessed test dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the raw dataset\n",
    "    raw_datasets = load_dataset(path=ds_path, name=ds_name)\n",
    "    \n",
    "    # 1. Test dataset\n",
    "    raw_test_dataset = test_dataset_split(raw_datasets, ds_path)\n",
    "    # 2. Training dataset\n",
    "    raw_train_dataset = raw_datasets['train']\n",
    "\n",
    "    # Calculate the prior distribution\n",
    "    prior_distribution, num_labels = calculate_prior_distribution(raw_train_dataset)\n",
    "\n",
    "    # Partition the dataset into federated datasets\n",
    "    client_datasets = federated_dirichlet_datasets(raw_train_dataset, prior_distribution, num_clients, alpha)\n",
    "\n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    \n",
    "    # Create the tokenization function\n",
    "    tokenize_fn = tokenize_function(ds_path, ds_name, tokenizer)\n",
    "\n",
    "    # Tokenize the client datasets\n",
    "    tok_client_datasets = tokenize_client_datasets(client_datasets, tokenize_fn)\n",
    "\n",
    "    # Create DataLoaders for each client dataset\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    client_dataloaders = create_client_dataloaders(tok_client_datasets, batch_size, data_collator)\n",
    "    \n",
    "    fed_ds = FederatedDataset(client_dataloaders)\n",
    "\n",
    "    # Preprocess the test dataset\n",
    "    test_ds = preprocess_test_dataset(raw_test_dataset, tokenize_fn, data_collator, batch_size)\n",
    "\n",
    "    return fed_ds, test_ds\n",
    "\n",
    "\n",
    "class FederatedDataset:\n",
    "    \"\"\"\n",
    "    A class to handle federated datasets for training in a federated learning setup.\n",
    "\n",
    "    This class encapsulates the logic for managing multiple client dataloaders and providing\n",
    "    batched data for federated learning training loops.\n",
    "\n",
    "    Args:\n",
    "        client_dataloaders (list of DataLoader): A list of DataLoader objects, each corresponding to a client's dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, client_dataloaders):\n",
    "        self.client_dataloaders = client_dataloaders\n",
    "        self.client_batch_generators = [\n",
    "            self.dataloader_batch_generator(client_dl)\n",
    "            for client_dl in client_dataloaders\n",
    "        ]\n",
    "    \n",
    "    def epoch_steps(self, client_ids):\n",
    "        \"\"\"\n",
    "        Determine the number of steps (batches) in an epoch for the given clients.\n",
    "\n",
    "        Args:\n",
    "            client_ids (list of int): A list of client IDs.\n",
    "\n",
    "        Returns:\n",
    "            int: The maximum number of steps (batches) for the given clients.\n",
    "        \"\"\"\n",
    "        \n",
    "        return max(\n",
    "            len(self.client_dataloaders[client_id])\n",
    "            for client_id in client_ids\n",
    "        )\n",
    "    \n",
    "    def next_client_batch(self, client_id):\n",
    "        \"\"\"\n",
    "        Retrieve the next batch of data for the specified client.\n",
    "\n",
    "        Args:\n",
    "            client_id (int): The ID of the client.\n",
    "\n",
    "        Returns:\n",
    "            dict: A batch of data from the client's DataLoader.\n",
    "        \"\"\"\n",
    "        \n",
    "        return next(\n",
    "            self.client_batch_generators[client_id]\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def dataloader_batch_generator(dataloader):\n",
    "        \"\"\"\n",
    "        A generator that yields (unending) batches of data from a DataLoader.\n",
    "\n",
    "        Args:\n",
    "            dataloader (DataLoader): A DataLoader object.\n",
    "\n",
    "        Yields:\n",
    "            dict: A batch of data from the DataLoader.\n",
    "        \"\"\"\n",
    "        \n",
    "        while True:\n",
    "            for batch in dataloader:\n",
    "                yield batch\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f224c0c4-789f-44d5-9764-ae0839be421b",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7c476ea-de05-401a-9ef8-1939dba106de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW, SGD\n",
    "\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c35571c-f7ad-44b7-afe6-0925ce58e38a",
   "metadata": {},
   "source": [
    "## Misc|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eea696d8-c62b-4b03-9c88-25d93126a60c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def copy_parameters(from_params, to_params):\n",
    "    \"\"\"\n",
    "    Copies the values from one set of parameters to another.\n",
    "\n",
    "    This function operates in-place and modifies the `to_parameters` directly.\n",
    "    The @torch.no_grad() decorator ensures that this operation is not tracked \n",
    "    by autograd, preventing unnecessary computation and memory usage.\n",
    "\n",
    "    Args:\n",
    "        from_parameters (list of torch.nn.Parameter): An iterable of source parameters to copy from.\n",
    "        to_parameters (list of torch.nn.Parameter): An iterable of destination parameters to copy to.\n",
    "    \"\"\"\n",
    "    for from_param, to_param in zip(from_params, to_params):\n",
    "        to_param.copy_(from_param)\n",
    "        \n",
    "    \n",
    "@torch.no_grad\n",
    "def average_client_parameters(client_train_params):\n",
    "    \"\"\"\n",
    "    Averages the parameters from multiple clients.\n",
    "\n",
    "    This function computes the mean of the parameters from all clients. It stacks\n",
    "    the parameters for each layer across clients, computes the mean, and returns\n",
    "    the averaged parameters. The @torch.no_grad decorator ensures that this operation\n",
    "    is not tracked by autograd.\n",
    "\n",
    "    Args:\n",
    "        client_train_params (dict): A dictionary where keys are client IDs and values are lists of parameter tensors.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of averaged parameters.\n",
    "    \"\"\"\n",
    "    average_params = [\n",
    "        torch.mean(torch.stack(param_list), dim=0)\n",
    "        for param_list in zip(*client_train_params.values())\n",
    "    ]\n",
    "    \n",
    "    return average_params\n",
    "\n",
    "\n",
    "@torch.no_grad\n",
    "def compute_metrics(model, ds_path, ds_name, test_ds):\n",
    "    \"\"\"\n",
    "    Computes evaluation metrics for the given model on the test dataset.\n",
    "\n",
    "    This function evaluates the model on the provided test dataset and computes\n",
    "    metrics using the `evaluate` library. The `@torch.no_grad` decorator ensures\n",
    "    that the evaluation is performed without tracking gradients, which saves memory\n",
    "    and computation.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to be evaluated.\n",
    "        ds_path (str): The path or identifier of the dataset. This is used to load the appropriate evaluation metric.\n",
    "        ds_name (str): The name of the specific dataset configuration. This helps in loading the correct evaluation metric.\n",
    "        test_ds (DataLoader): A DataLoader for the test dataset.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the computed evaluation metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the evaluation metric\n",
    "    metric = evaluate.load(path=ds_path, config_name=ds_name)\n",
    "    \n",
    "    testing_loss = 0.0\n",
    "    num_batches = len(test_ds)\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    for batch in test_ds:\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        \n",
    "        # Perform a forward pass\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Get logits and predictions\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        # Add batch predictions and references to the metric\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        \n",
    "        testing_loss += loss.item()\n",
    "        \n",
    "    # Calculate the average test loss\n",
    "    average_test_loss = testing_loss / num_batches\n",
    "        \n",
    "    # Compute the final evaluation metrics\n",
    "    metrics = metric.compute()\n",
    "        \n",
    "    # Add the average test loss to the evaluation metrics\n",
    "    metrics['testing_loss'] = average_test_loss\n",
    "    \n",
    "    # Compute and return the final evaluation metrics\n",
    "    return metrics\n",
    "\n",
    "\n",
    "@torch.no_grad\n",
    "def compute_drifts(old_params, new_params):\n",
    "    \"\"\"\n",
    "    Calculate the drift (difference) between old and new parameters.\n",
    "\n",
    "    Args:\n",
    "        old_params (list of torch.nn.Parameter): The original parameters.\n",
    "        new_params (list of torch.nn.Parameter): The updated parameters.\n",
    "\n",
    "    Returns:\n",
    "        list of torch.Tensor: The computed drifts for each parameter.\n",
    "    \"\"\"\n",
    "    \n",
    "    return [\n",
    "        new_param - old_param\n",
    "        for old_param, new_param in zip(old_params, new_params)\n",
    "    ]\n",
    "\n",
    "@torch.no_grad\n",
    "def compute_client_drifts(old_params, client_train_params):\n",
    "    \"\"\"\n",
    "    Compute the drifts for all clients based on the original parameters.\n",
    "\n",
    "    Args:\n",
    "        old_params (list of torch.nn.Parameter): The original parameters.\n",
    "        client_train_params (dict): Dictionary of client IDs and their corresponding parameters.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are client IDs and values are lists of drifts for each parameter.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        client_id: compute_drifts(old_params, client_params) \n",
    "        for client_id, client_params in client_train_params.items()\n",
    "    }\n",
    "\n",
    "@torch.no_grad\n",
    "def compute_pseudo_gradients(client_drifts):\n",
    "    \"\"\"\n",
    "    Compute the pseudo-gradient based on the drifts between old and client parameters.\n",
    "\n",
    "    Args:\n",
    "        client_drifts (dict): Dictionary of client IDs and their corresponding parameter drifts.\n",
    "\n",
    "    Returns:\n",
    "        list of torch.Tensor: The computed pseudo-gradient.\n",
    "    \"\"\"\n",
    "    average_drifts = average_client_parameters(client_drifts)\n",
    "    \n",
    "    pseudo_gradients = [-drift for drift in average_drifts]\n",
    "    \n",
    "    return pseudo_gradients\n",
    "\n",
    "\n",
    "@torch.no_grad\n",
    "def set_gradients(train_params, gradients):\n",
    "    \"\"\"\n",
    "    Set gradients for trainable parameters.\n",
    "\n",
    "    This function assigns the provided gradients to the .grad attribute of the corresponding trainable parameters.\n",
    "\n",
    "    Args:\n",
    "        train_params (list of torch.nn.Parameter): The trainable parameters of the model.\n",
    "        gradients (list of torch.Tensor): The gradients to be assigned to the parameters.\n",
    "    \"\"\"\n",
    "    for param, gradient in zip(train_params, gradients):\n",
    "        param.grad = gradient\n",
    "        \n",
    "\n",
    "class ClientSampler:\n",
    "    \"\"\"\n",
    "    A class that helps random sampling of clients in each round.\n",
    "\n",
    "    Attributes:\n",
    "        client_ids (list): A list of client IDs to be sampled.\n",
    "        sample_size (int): The size of each group of sampled client IDs.\n",
    "        num_clients (int): The total number of clients.\n",
    "        client_sample_generator (generator): A generator that yields groups of client IDs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, client_ids, sample_size):\n",
    "        self.client_ids = client_ids\n",
    "        self.sample_size = sample_size\n",
    "        self.num_clients = len(client_ids)\n",
    "        \n",
    "        if self.num_clients % self.sample_size != 0:\n",
    "            raise ValueError(\"The number of clients must be divisible with the sample size.\")\n",
    "        \n",
    "        self.client_sample_generator = self.sample_generator()\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Returns the next group of sampled client IDs.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of sampled client IDs of length `sample_size`.\n",
    "        \"\"\"\n",
    "        return next(self.client_sample_generator)\n",
    "        \n",
    "    def sample_generator(self):\n",
    "        \"\"\"\n",
    "        A generator function to continuously generate groups of sampled client IDs.\n",
    "\n",
    "        This function shuffles the client IDs and partitions them into groups of the specified sample size.\n",
    "        It yields each group of sampled client IDs.\n",
    "\n",
    "        Yields:\n",
    "            list: A list of sampled client IDs of length `sample_size`.\n",
    "        \"\"\"\n",
    "        \n",
    "        while True:\n",
    "            # Shuffle client_ids without modifying the original list https://stackoverflow.com/a/47750824\n",
    "            shuffled_client_ids = random.sample(self.client_ids, self.num_clients)\n",
    "\n",
    "            # Partition the shuffled list into groups of the specified sample size\n",
    "            groups = [shuffled_client_ids[i:i + self.sample_size] for i in range(0, self.num_clients, self.sample_size)]\n",
    "\n",
    "            # Yield each group of sampled client IDs\n",
    "            for sampled_client_ids in groups:\n",
    "                yield sampled_client_ids\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80c270f-14b7-417c-8f14-3dbc16e0575b",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a3a34b7-9a59-407e-b5de-8b0c7a1cbe15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def federated_training_step(model, train_params, client_train_params, client_opt, fed_ds):\n",
    "    \"\"\"\n",
    "    Performs a single federated training step.\n",
    "\n",
    "    This function trains each client starting with its client-specific model parameters, updates them\n",
    "    with the client optimizer and client-specific batch, and returns. At the end, client_train_params\n",
    "    have the updated client-specific parameters (after training on their specific batch).\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to be trained.\n",
    "        train_params (list): A list of trainable parameters of the model.\n",
    "        client_train_params (dict): A dictionary where keys are client IDs and values are lists of parameter tensors.\n",
    "        client_opt (torch.optim.Optimizer): The optimizer for updating the model parameters.\n",
    "        fed_ds (FederatedDataset): An object that provides the batches for each client.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    training_loss = 0.0\n",
    "    num_clients = len(client_train_params)\n",
    "    \n",
    "    # Iterate over each client\n",
    "    for client_id in client_train_params.keys():\n",
    "\n",
    "        # Copy client-specific parameters to the model's parameters\n",
    "        copy_parameters(\n",
    "            from_params=client_train_params[client_id], \n",
    "            to_params=train_params\n",
    "        )\n",
    "\n",
    "        # Get the next batch of data for the current client\n",
    "        batch = fed_ds.next_client_batch(client_id)\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "\n",
    "        # Perform a forward pass\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass: compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        client_opt.step()\n",
    "\n",
    "        # Copy the updated model parameters back to the client's parameter set\n",
    "        copy_parameters(\n",
    "            from_params=train_params, \n",
    "            to_params=client_train_params[client_id]\n",
    "        )\n",
    "        \n",
    "        # Zero the gradients before the next backward pass\n",
    "        client_opt.zero_grad()\n",
    "        \n",
    "        # Accumulate the loss\n",
    "        training_loss += loss.item()\n",
    "    \n",
    "    # Calculate the average loss\n",
    "    training_loss = training_loss / num_clients\n",
    "    \n",
    "    return training_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aa1c94-415e-44d4-8234-61b60ba91fe4",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85e335b4-4cbc-4949-9649-b46a2a274aa7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457ff9beb2af45c580a422a8d1b301fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/37 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "ds_path, ds_name = \"glue\", \"mrpc\"\n",
    "num_clients = 100\n",
    "alpha = 1\n",
    "batch_size = 8\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "\n",
    "clients_per_round = 10\n",
    "local_epochs = 1\n",
    "\n",
    "# Prepare federated datasets and DataLoaders\n",
    "fed_ds, test_ds = prepare_federated_datasets(ds_path, ds_name, checkpoint, num_clients, alpha, batch_size)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Extract trainable parameters from the model, which reside on the device that the model resides in\n",
    "train_params = [param for param in model.parameters() if param.requires_grad]\n",
    "\n",
    "# Create a copy of the trainable parameters, detached from the computation graph and moved to the CPU\n",
    "# TODO: This hardcodes 'cpu' as the target device for the detached parameters\n",
    "round_start_train_params = [param.detach().clone() for param in train_params]\n",
    "\n",
    "#server_opt = AdamW(train_params, lr=0.0005, weight_decay=0.001)\n",
    "server_opt = SGD(train_params, lr=1)\n",
    "client_opt = SGD(train_params, lr=0.001)  # Note: One optimizer because SGD is stateless.\n",
    "\n",
    "client_ids = list(range(num_clients))\n",
    "\n",
    "client_sampler = ClientSampler(client_ids, clients_per_round)\n",
    "\n",
    "metrics_lst = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4648bb9-337f-4655-8b58-5bca769b7132",
   "metadata": {},
   "source": [
    "### Round Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05f2a569-d9c6-4e4f-b2a8-a5daf77da741",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round: 0 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6223561494958167, 'training_loss': 0.43800547391176226}\n",
      "0.008778114803135395\n",
      "round: 1 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6230824613103679, 'training_loss': 0.42960845425724975}\n",
      "0.008828705176711082\n",
      "round: 2 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6221566235317904, 'training_loss': 0.4847204847633838}\n",
      "0.00884535163640976\n",
      "round: 3 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6252190920652128, 'training_loss': 0.4207545524835587}\n",
      "0.008853411301970482\n",
      "round: 4 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6350316218301362, 'training_loss': 0.5905634710192681}\n",
      "0.008867672644555569\n",
      "round: 5 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.668480705981161, 'training_loss': 0.4167860317230224}\n",
      "0.008906270377337933\n",
      "round: 6 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6258362858903175, 'training_loss': 0.6322038051486015}\n",
      "0.00892652291804552\n",
      "round: 7 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6389488314881044, 'training_loss': 0.39694297313690186}\n",
      "0.008933908306062222\n",
      "round: 8 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.622168597053079, 'training_loss': 0.5100289949774742}\n",
      "0.008956140838563442\n",
      "round: 9 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6369253280116063, 'training_loss': 0.5706722308695316}\n",
      "0.008956873789429665\n",
      "round: 10 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6385973130955416, 'training_loss': 0.5197284808754921}\n",
      "0.008945000357925892\n",
      "round: 11 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6237793225868076, 'training_loss': 0.5111741754412652}\n",
      "0.008987193927168846\n",
      "round: 12 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6272493764465931, 'training_loss': 0.4300807851552964}\n",
      "0.008907107636332512\n",
      "round: 13 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6582977342254975, 'training_loss': 0.4114669869840145}\n",
      "0.008946368470788002\n",
      "round: 14 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6510987971343246, 'training_loss': 0.40051626056432726}\n",
      "0.008943802677094936\n",
      "round: 15 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.657322333723891, 'training_loss': 0.3781067414581776}\n",
      "0.008979727514088154\n",
      "round: 16 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6246257471103295, 'training_loss': 0.5817617750167847}\n",
      "0.008948592469096184\n",
      "round: 17 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6403079044585135, 'training_loss': 0.520891816318035}\n",
      "0.008933676406741142\n",
      "round: 18 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6226585133402955, 'training_loss': 0.4664521971344948}\n",
      "0.008949967101216316\n",
      "round: 19 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6452335130934622, 'training_loss': 0.521168161034584}\n",
      "0.008901497349143028\n",
      "round: 20 metrics: {'accuracy': 0.3161764705882353, 'f1': 0.0, 'testing_loss': 0.7055883559526182, 'training_loss': 0.5724343726038932}\n",
      "0.00895538367331028\n",
      "round: 21 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6281326196941674, 'training_loss': 0.504273673593998}\n",
      "0.008963916450738907\n",
      "round: 22 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6456232713718041, 'training_loss': 0.5319671881198883}\n",
      "0.008918697014451027\n",
      "round: 23 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6302952620328641, 'training_loss': 0.5259027716517448}\n",
      "0.008946648798882961\n",
      "round: 24 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6364615185588014, 'training_loss': 0.4675724796950817}\n",
      "0.008918683044612408\n",
      "round: 25 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6652381452275258, 'training_loss': 0.3998638796806335}\n",
      "0.008973394520580769\n",
      "round: 26 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6505882719568178, 'training_loss': 0.44942501485347747}\n",
      "0.008896252140402794\n",
      "round: 27 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.7236012083058264, 'training_loss': 0.309921373873949}\n",
      "0.008922490291297436\n",
      "round: 28 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6306283555778802, 'training_loss': 0.6637215420603753}\n",
      "0.008881662972271442\n",
      "round: 29 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6673926491947735, 'training_loss': 0.47086453437805176}\n",
      "0.008968541398644447\n",
      "round: 30 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6282594870118534, 'training_loss': 0.4688590502738953}\n",
      "0.008943421766161919\n",
      "round: 31 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6216536725268644, 'training_loss': 0.5071427658200264}\n",
      "0.008908336982131004\n",
      "round: 32 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6274917341914832, 'training_loss': 0.5519905909895898}\n",
      "0.0089649623259902\n",
      "round: 33 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6423390589508355, 'training_loss': 0.5477609857916832}\n",
      "0.008940834552049637\n",
      "round: 34 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6498865455973382, 'training_loss': 0.41283855587244034}\n",
      "0.008945932611823082\n",
      "round: 35 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6307019889354706, 'training_loss': 0.5030034294724464}\n",
      "0.008908345364034176\n",
      "round: 36 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6293758755805445, 'training_loss': 0.4448224052786827}\n",
      "0.008982992731034756\n",
      "round: 37 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6267583177370184, 'training_loss': 0.4757150687277317}\n",
      "0.008915324695408344\n",
      "round: 38 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6529565532417858, 'training_loss': 0.42897798418998717}\n",
      "0.008967887610197067\n",
      "round: 39 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6229087909062704, 'training_loss': 0.5114731027185917}\n",
      "0.008918959647417068\n",
      "round: 40 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6258582268275467, 'training_loss': 0.4845462691783905}\n",
      "0.00897274911403656\n",
      "round: 41 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6206735758220449, 'training_loss': 0.47910739004611963}\n",
      "0.008956724777817726\n",
      "round: 42 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6468757639328638, 'training_loss': 0.4296537384390831}\n",
      "0.008943290449678898\n",
      "round: 43 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6288801852394553, 'training_loss': 0.48825754478573796}\n",
      "0.008943346329033375\n",
      "round: 44 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6223756372928619, 'training_loss': 0.49782759875059135}\n",
      "0.008967520669102669\n",
      "round: 45 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6281052973924899, 'training_loss': 0.4658800630271435}\n",
      "0.008943459950387478\n",
      "round: 46 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.620857730215671, 'training_loss': 0.48017317801713943}\n",
      "0.008941645734012127\n",
      "round: 47 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6536732727990431, 'training_loss': 0.40665709733963007}\n",
      "0.008944198489189148\n",
      "round: 48 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6431739412102044, 'training_loss': 0.6094378717243673}\n",
      "0.008935486897826195\n",
      "round: 49 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6278339542594611, 'training_loss': 0.5213239336013793}\n",
      "0.008901778608560562\n",
      "round: 50 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6277192947911281, 'training_loss': 0.5377345210313796}\n",
      "0.008953279815614223\n",
      "round: 51 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6212100994353201, 'training_loss': 0.5008481875061989}\n",
      "0.008945843204855919\n",
      "round: 52 metrics: {'accuracy': 0.6348039215686274, 'f1': 0.7025948103792415, 'testing_loss': 0.6911239729208105, 'training_loss': 0.5499634456634521}\n",
      "0.008956105448305607\n",
      "round: 53 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.636570930480957, 'training_loss': 0.43546074271202084}\n",
      "0.008964039385318756\n",
      "round: 54 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6207900281045952, 'training_loss': 0.5583078241348266}\n",
      "0.008934034034609795\n",
      "round: 55 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6447437347150317, 'training_loss': 0.5214488717913628}\n",
      "0.008940309286117554\n",
      "round: 56 metrics: {'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6265730729290083, 'training_loss': 0.44200871765613564}\n",
      "0.008965750224888325\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 57\u001b[0m\n\u001b[1;32m     49\u001b[0m server_opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#copy_parameters(\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m#    from_params=average_train_params, \u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m#    to_params=train_params\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Calculate evaluation metrics on the test set\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_ds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Calculate the average training loss for the round\u001b[39;00m\n\u001b[1;32m     60\u001b[0m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_loss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m training_loss \u001b[38;5;241m/\u001b[39m epoch_steps\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 63\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(model, ds_path, ds_name, test_ds)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03mComputes evaluation metrics for the given model on the test dataset.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    dict: A dictionary containing the computed evaluation metrics.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Load the evaluation metric\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m metric \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m testing_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     66\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(test_ds)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/evaluate/loading.py:748\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, config_name, module_type, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, revision, **init_kwargs)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a [`~evaluate.EvaluationModule`].\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \n\u001b[1;32m    705\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    747\u001b[0m download_mode \u001b[38;5;241m=\u001b[39m DownloadMode(download_mode \u001b[38;5;129;01mor\u001b[39;00m DownloadMode\u001b[38;5;241m.\u001b[39mREUSE_DATASET_IF_EXISTS)\n\u001b[0;32m--> 748\u001b[0m evaluation_module \u001b[38;5;241m=\u001b[39m \u001b[43mevaluation_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m evaluation_cls \u001b[38;5;241m=\u001b[39m import_main_class(evaluation_module\u001b[38;5;241m.\u001b[39mmodule_path)\n\u001b[1;32m    752\u001b[0m evaluation_instance \u001b[38;5;241m=\u001b[39m evaluation_cls(\n\u001b[1;32m    753\u001b[0m     config_name\u001b[38;5;241m=\u001b[39mconfig_name,\n\u001b[1;32m    754\u001b[0m     process_id\u001b[38;5;241m=\u001b[39mprocess_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs,\n\u001b[1;32m    761\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/evaluate/loading.py:633\u001b[0m, in \u001b[0;36mevaluation_module_factory\u001b[0;34m(path, module_type, revision, download_config, download_mode, force_local_path, dynamic_modules_path, **download_kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m current_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomparison\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeasurement\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 633\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mHubEvaluationModuleFactory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevaluate-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcurrent_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpath\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdynamic_modules_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_modules_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget_module()\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/evaluate/loading.py:462\u001b[0m, in \u001b[0;36mHubEvaluationModuleFactory.__init__\u001b[0;34m(self, name, module_type, revision, download_config, download_mode, dynamic_modules_path)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic_modules_path \u001b[38;5;241m=\u001b[39m dynamic_modules_path\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 462\u001b[0m \u001b[43mincrease_load_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetric\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/evaluate/loading.py:134\u001b[0m, in \u001b[0;36mincrease_load_count\u001b[0;34m(name, resource_type)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mHF_EVALUATE_OFFLINE \u001b[38;5;129;01mand\u001b[39;00m config\u001b[38;5;241m.\u001b[39mHF_UPDATE_DOWNLOAD_COUNTS:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 134\u001b[0m         \u001b[43mhead_hf_s3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresource_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/evaluate/utils/file_utils.py:93\u001b[0m, in \u001b[0;36mhead_hf_s3\u001b[0;34m(identifier, filename, use_cdn, dataset, max_retries)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhead_hf_s3\u001b[39m(\n\u001b[1;32m     91\u001b[0m     identifier: \u001b[38;5;28mstr\u001b[39m, filename: \u001b[38;5;28mstr\u001b[39m, use_cdn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     92\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[requests\u001b[38;5;241m.\u001b[39mResponse, \u001b[38;5;167;01mException\u001b[39;00m]:\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhttp_head\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_bucket_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43midentifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midentifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cdn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cdn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/evaluate/utils/file_utils.py:378\u001b[0m, in \u001b[0;36mhttp_head\u001b[0;34m(url, proxies, headers, cookies, allow_redirects, timeout, max_retries)\u001b[0m\n\u001b[1;32m    376\u001b[0m headers \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(headers) \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    377\u001b[0m headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser-agent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m get_datasets_user_agent(user_agent\u001b[38;5;241m=\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser-agent\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m--> 378\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcookies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/evaluate/utils/file_utils.py:307\u001b[0m, in \u001b[0;36m_request_with_retry\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, **params)\u001b[0m\n\u001b[1;32m    305\u001b[0m tries \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 307\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectTimeout, requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectionError) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    468\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1095\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mproxy_is_verified:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connection.py:652\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;66;03m# Remove trailing '.' from fqdn hostnames to allow certificate validation\u001b[39;00m\n\u001b[1;32m    650\u001b[0m server_hostname_rm_dot \u001b[38;5;241m=\u001b[39m server_hostname\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 652\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n\u001b[1;32m    672\u001b[0m \u001b[38;5;66;03m# Forwarding proxies can never have a verified target since\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;66;03m# the proxy is the one doing the verification. Should instead\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;66;03m# use a CONNECT tunnel in order to verify the target.\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;66;03m# See: https://github.com/urllib3/urllib3/issues/3267.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/connection.py:805\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[0;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_ipaddress(normalized):\n\u001b[1;32m    803\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[0;32m--> 805\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m assert_fingerprint:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/util/ssl_.py:465\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:  \u001b[38;5;66;03m# Defensive: in CI, we always have set_alpn_protocols\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/urllib3/util/ssl_.py:509\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    506\u001b[0m     SSLTransport\u001b[38;5;241m.\u001b[39m_validate_ssl_context_for_tls_in_tls(ssl_context)\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[0;32m--> 509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/ssl.py:455\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    450\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    451\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    452\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/ssl.py:1042\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m   1040\u001b[0m                 \u001b[38;5;66;03m# non-blocking\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1042\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/ssl.py:1320\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m block:\n\u001b[1;32m   1319\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1320\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for r in range(1000):\n",
    "    \n",
    "    training_loss = 0.0\n",
    "\n",
    "    # Save the model parameters at the start of this round\n",
    "    sampled_clients = client_sampler.sample() \n",
    "    \n",
    "    # Save the model parameters at the start of this round\n",
    "    copy_parameters(\n",
    "        from_params=train_params,\n",
    "        to_params=round_start_train_params\n",
    "    )\n",
    "\n",
    "    # Initialize a dictionary to store the trainable parameters for each client\n",
    "    # Each client's parameters are cloned from the round start parameters\n",
    "    client_train_params = {\n",
    "        client_id: [param.detach().clone() for param in round_start_train_params]\n",
    "        for client_id in sampled_clients\n",
    "    }\n",
    "\n",
    "    # Calculate the total number of steps for this epoch\n",
    "    epoch_steps = local_epochs * fed_ds.epoch_steps(sampled_clients)\n",
    "\n",
    "    for step in range(epoch_steps):\n",
    "        # Perform a federated training step and accumulate the training loss\n",
    "        training_loss += federated_training_step(model, train_params, client_train_params, client_opt, fed_ds)\n",
    "        \n",
    "    # Reset the model parameters to the parameters at the start of the round. This ensures that the \n",
    "    # server-side optimizer updates are applied correctly (on the parameters at the start of the round)\n",
    "    copy_parameters(\n",
    "        from_params=round_start_train_params,\n",
    "        to_params=train_params\n",
    "    )\n",
    "    \n",
    "    # Compute the drifts (differences) between the round start parameters and the client parameters\n",
    "    client_drifts = compute_client_drifts(round_start_train_params, client_train_params)\n",
    "    \n",
    "    # Compute pseudo-gradients based on the average drifts\n",
    "    pseudo_gradients = compute_pseudo_gradients(client_drifts)\n",
    "    \n",
    "    # Set the computed pseudo-gradients to the trainable parameters\n",
    "    set_gradients(train_params, pseudo_gradients)\n",
    "    \n",
    "    # Update model parameters (global model) using the server optimizer based on pseudo-gradients\n",
    "    server_opt.step()\n",
    "    \n",
    "    # Zero the gradients before the next backward pass\n",
    "    server_opt.zero_grad()\n",
    "    \n",
    "    # Calculate evaluation metrics on the test set\n",
    "    metrics = compute_metrics(model, ds_path, ds_name, test_ds)\n",
    "    \n",
    "    # Calculate the average training loss for the round\n",
    "    metrics['training_loss'] = training_loss / epoch_steps\n",
    "    \n",
    "    metrics_lst.append(metrics)\n",
    "    \n",
    "    print(f\"round: {r} metrics: {metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aafd97e-ada6-49cb-babe-b7aea084b57b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda431c6-4ab7-46bf-b2e4-ad3bd73f1cac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f038a3-c119-41ee-8e7f-fa880d47aa64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28be2d81-9c6d-41c9-9dda-154a6088a7d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ae79619-2106-45d9-99b0-6c2e131c59fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079, 'testing_loss': 0.6399823152551464, 'training_loss': 0.43800547391176226}\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(metrics_lst[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1af10d-429a-4b2f-9d94-5da26183bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the filename\n",
    "filename = \"output.txt\"\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(filename, 'w') as file:\n",
    "    # Iterate over the list and write each string to the file\n",
    "    for item in metrics_lst:\n",
    "        file.write(str(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d476a701-c418-4b64-acb4-7056104b0a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
