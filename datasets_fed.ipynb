{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4133e93-fd30-443b-a196-d346fab894b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets.utils.logging import set_verbosity_error\n",
    "set_verbosity_error()\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def calculate_prior_distribution(raw_train_dataset):\n",
    "    \"\"\"\n",
    "    Calculate the prior distribution of labels in the training dataset.\n",
    "\n",
    "    This function calculates the proportion of each label in the training dataset, \n",
    "    which is used as the prior distribution for Dirichlet sampling.\n",
    "\n",
    "    Args:\n",
    "        raw_train_dataset (datasets.arrow_dataset.Dataset): The training dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - prior_distribution (numpy.ndarray): An array of label proportions.\n",
    "            - num_labels (int): The number of unique labels in the dataset.\n",
    "    \"\"\"\n",
    "        \n",
    "    # Calculate the number of samples for each label\n",
    "    label_count_dict = dict(raw_train_dataset.to_pandas()['label'].value_counts())\n",
    "    # Num. of samples\n",
    "    n = len(raw_train_dataset) \n",
    "    # (label, count) list, sorted with label increasing\n",
    "    label_count_tuple = sorted(label_count_dict.items(), key=lambda x: x[0])\n",
    "    # prior distribution\n",
    "    prior_distribution = np.array([c / n for _, c in label_count_tuple])\n",
    "    \n",
    "    # number of labels\n",
    "    num_labels = len(label_count_dict)\n",
    "    \n",
    "    return prior_distribution, num_labels\n",
    "\n",
    "\n",
    "def sample_to_client(client_sample_counts, sample_label):\n",
    "    \"\"\"\n",
    "    Assign a sample to a client based on the sample's label and the remaining counts for each client.\n",
    "\n",
    "    This function assigns a sample to the client that still needs more samples of the given label.\n",
    "    It decrements the count for that label for the chosen client. If no client has a remaining count\n",
    "    of at least 1 for the label, the function assigns the sample to the client with the largest leftover count\n",
    "    for that label.\n",
    "\n",
    "    Args:\n",
    "        client_sample_counts (numpy.ndarray): A 2D array where each row corresponds to a client and each column corresponds\n",
    "            to the count (float) of samples needed for each label. The element at (i, j) represents the decimal number of \n",
    "            samples of label j that client i still needs.\n",
    "        sample_label (int): The label of the sample to be assigned to a client.\n",
    "        \n",
    "    Returns:\n",
    "        int: The index of the client to which the sample has been assigned.\n",
    "    \"\"\"\n",
    "    num_clients = len(client_sample_counts)\n",
    "    client_indices = np.random.permutation(num_clients)\n",
    "    \n",
    "    for client_idx in client_indices:\n",
    "        client_sample_count = client_sample_counts[client_idx]\n",
    "        \n",
    "        if client_sample_count[sample_label] >= 1:\n",
    "            client_sample_count[sample_label] -= 1\n",
    "            \n",
    "            return client_idx\n",
    "    \n",
    "    # if all client data counts are less than 1, then assign the sample to the largest leftover\n",
    "    client_idx = np.argmax(client_sample_counts[:, sample_label])\n",
    "    client_sample_counts[client_idx][sample_label] -= 1\n",
    "    \n",
    "    return client_idx\n",
    "\n",
    "\n",
    "def federated_dirichlet_datasets(raw_train_dataset, prior_distribution, num_clients, alpha):\n",
    "    \"\"\"\n",
    "    Create federated datasets using Dirichlet-distributed label partitions.\n",
    "\n",
    "    This function partitions the training dataset into multiple subsets, each corresponding \n",
    "    to a client. The label distribution for each client is drawn from a Dirichlet distribution \n",
    "    parameterized by the given prior distribution and concentration parameter alpha.\n",
    "    The function also aims to keep the datasets as equal in size as possible.\n",
    "\n",
    "    Args:\n",
    "        raw_train_dataset (datasets.arrow_dataset.Dataset): The training dataset.\n",
    "        prior_distribution (numpy.ndarray): An array representing the prior distribution of labels.\n",
    "        num_clients (int): The number of clients.\n",
    "        alpha (float): The concentration parameter for the Dirichlet distribution.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of `datasets.arrow_dataset.Dataset` objects, each representing a client's dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Num. of samples\n",
    "    n = len(raw_train_dataset)\n",
    "    \n",
    "    client_num_samples = n / num_clients\n",
    "    \n",
    "    # Draw label distributions for each client from Dirichlet distribution\n",
    "    # Each i-th row represents the distribution of labels for the i-th client\n",
    "    client_distributions = np.random.dirichlet(alpha * prior_distribution, num_clients)\n",
    "    \n",
    "    # Initialize client data indices\n",
    "    client_sample_indices = [[] for i in range(num_clients)]\n",
    "    \n",
    "    # Calculate the number of samples per label each client should have\n",
    "    # client_sample_counts[i] is an array of `num_label` elements\n",
    "    # -- the counts for each label for the i-th client.\n",
    "    client_sample_counts = np.array([\n",
    "        client_distributions[client_idx, :] * client_num_samples\n",
    "        for client_idx in range(num_clients)\n",
    "    ])\n",
    "    \n",
    "    for sample_idx, sample in enumerate(raw_train_dataset):\n",
    "        client_idx = sample_to_client(client_sample_counts, sample['label'])\n",
    "\n",
    "        client_sample_indices[client_idx].append(sample_idx)\n",
    "        \n",
    "    # Create a Dataset for each client\n",
    "    client_datasets = []\n",
    "    for client_indices in client_sample_indices:\n",
    "        client_dataset = raw_train_dataset.select(client_indices)\n",
    "        client_datasets.append(client_dataset)\n",
    "        \n",
    "    return client_datasets\n",
    "\n",
    "\n",
    "def tokenize_client_datasets(client_datasets, tokenize_fn):\n",
    "    \"\"\"\n",
    "    Tokenize and preprocess a list of client datasets.\n",
    "\n",
    "    This function tokenizes and preprocesses each dataset in the provided list of client datasets.\n",
    "    It applies the specified tokenization function, renames the \"label\" column to \"labels\",\n",
    "    removes unnecessary columns, and sets the format to PyTorch tensors.\n",
    "\n",
    "    Args:\n",
    "        client_datasets (list of datasets.arrow_dataset.Dataset): A list of client datasets to be tokenized and preprocessed.\n",
    "        tokenization_fn (function): A function that takes an example and returns its tokenized form.\n",
    "\n",
    "    Returns:\n",
    "        list of datasets.arrow_dataset.Dataset: A list of tokenized and preprocessed client datasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the expected columns\n",
    "    expected_columns = ['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n",
    "    \n",
    "    tok_client_datasets = []\n",
    "\n",
    "    for client_dataset in client_datasets:\n",
    "        tok_client_dataset = client_dataset.map(tokenize_fn, batched=True)\n",
    "\n",
    "        tok_client_dataset = tok_client_dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "        # Identify columns to remove\n",
    "        columns_to_remove = [\n",
    "            column for column in tok_client_dataset.column_names \n",
    "            if column not in expected_columns\n",
    "        ]\n",
    "\n",
    "        # Remove unnecessary columns\n",
    "        tok_client_dataset = tok_client_dataset.remove_columns(columns_to_remove)\n",
    "\n",
    "        # Set the format to PyTorch tensors\n",
    "        tok_client_dataset.set_format(\"torch\")\n",
    "\n",
    "        # Add the processed dataset to the list\n",
    "        tok_client_datasets.append(tok_client_dataset)\n",
    "        \n",
    "    return tok_client_datasets\n",
    "\n",
    "\n",
    "def preprocess_test_dataset(raw_test_dataset, tokenize_fn, data_collator, batch_size):\n",
    "    \"\"\"\n",
    "    Preprocess and tokenize the test dataset, then create a DataLoader for it.\n",
    "\n",
    "    This function tokenizes and preprocesses the provided test dataset using the specified\n",
    "    tokenization function. It renames the \"label\" column to \"labels\", removes unnecessary columns,\n",
    "    sets the format to PyTorch tensors, and then creates a DataLoader for the test dataset.\n",
    "\n",
    "    Args:\n",
    "        raw_test_dataset (datasets.arrow_dataset.Dataset): The raw test dataset to be tokenized and preprocessed.\n",
    "        tokenize_fn (function): A function that takes an example and returns its tokenized form.\n",
    "        data_collator (transformers.DataCollator): A data collator to be used for padding and batching.\n",
    "        batch_size (int): The batch size to be used by the DataLoader.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: A DataLoader for the tokenized and preprocessed test dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the expected columns\n",
    "    expected_columns = ['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n",
    "    \n",
    "    tok_test_dataset = raw_test_dataset.map(tokenize_fn, batched=True)\n",
    "    \n",
    "    tok_test_dataset = tok_test_dataset.rename_column(\"label\", \"labels\")\n",
    "    \n",
    "    # Identify columns to remove\n",
    "    columns_to_remove = [\n",
    "        column for column in tok_test_dataset.column_names \n",
    "        if column not in expected_columns\n",
    "    ]\n",
    "    \n",
    "    # Remove unnecessary columns\n",
    "    tok_test_dataset = tok_test_dataset.remove_columns(columns_to_remove)\n",
    "    \n",
    "    # Set the format to PyTorch tensors\n",
    "    tok_test_dataset.set_format(\"torch\")\n",
    "    \n",
    "    # Create a DataLoader for the test dataset\n",
    "    test_ds = DataLoader(\n",
    "        tok_test_dataset, batch_size=batch_size, collate_fn=data_collator\n",
    "    )\n",
    "        \n",
    "    return test_ds\n",
    "\n",
    "\n",
    "def create_client_dataloaders(tok_client_datasets, batch_size, collate_fn):\n",
    "    \"\"\"\n",
    "    Create data loaders for a list of tokenized client datasets.\n",
    "\n",
    "    This function takes a list of tokenized client datasets and creates a DataLoader for each dataset.\n",
    "    The resulting data loaders are stored in a list and returned.\n",
    "\n",
    "    Args:\n",
    "        tok_client_datasets (list of datasets.arrow_dataset.Dataset): A list of tokenized client datasets.\n",
    "        batch_size (int): The batch size to be used by the data loaders.\n",
    "        collate_fn (function): A collate function to be used by the data loaders.\n",
    "\n",
    "    Returns:\n",
    "        list of DataLoader: A list of data loaders, each corresponding to a tokenized client dataset.\n",
    "    \"\"\"\n",
    "    client_dataloaders = []\n",
    "\n",
    "    for tok_client_dataset in tok_client_datasets:\n",
    "        client_dataloader = DataLoader(\n",
    "            tok_client_dataset, shuffle=True, batch_size=batch_size, collate_fn=collate_fn\n",
    "        )\n",
    "        \n",
    "        client_dataloaders.append(client_dataloader)\n",
    "        \n",
    "    return client_dataloaders\n",
    "\n",
    "\n",
    "def tokenize_function(ds_path, ds_name, tokenizer):\n",
    "    \"\"\"\n",
    "    Return a tokenization function based on the dataset path and name.\n",
    "\n",
    "    This function returns the appropriate tokenization function for the specified dataset.\n",
    "    Currently, it supports the GLUE MRPC dataset.\n",
    "\n",
    "    Args:\n",
    "        ds_path (str): The path or identifier of the dataset.\n",
    "        ds_name (str): The name of the dataset.\n",
    "        tokenizer (AutoTokenizer): The tokenizer to be used.\n",
    "\n",
    "    Returns:\n",
    "        function: A tokenization function.\n",
    "    \"\"\"\n",
    "    \n",
    "    if ds_path == \"glue\" and ds_name == \"mrpc\":\n",
    "        return lambda example: tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def test_dataset_split(raw_datasets, ds_path):\n",
    "    \"\"\"\n",
    "    Split the raw dataset into the appropriate test set.\n",
    "\n",
    "    This function extracts the test set from the provided raw dataset based on the dataset path.\n",
    "    For the GLUE dataset, it returns the validation split as the test set following prior work.\n",
    "\n",
    "    Args:\n",
    "        raw_datasets (datasets.DatasetDict): The raw dataset containing multiple splits.\n",
    "        ds_path (str): The path or identifier of the dataset.\n",
    "\n",
    "    Returns:\n",
    "        datasets.Dataset: The extracted test dataset split.\n",
    "    \"\"\"\n",
    "    \n",
    "    if ds_path == 'glue':\n",
    "        return raw_datasets['validation']\n",
    "    \n",
    "\n",
    "def prepare_federated_datasets(ds_path, ds_name, checkpoint, num_clients, alpha, batch_size):\n",
    "    \"\"\"\n",
    "    Prepare federated datasets and create corresponding DataLoaders.\n",
    "\n",
    "    This function handles the entire process of loading the raw dataset, partitioning it into\n",
    "    training federated datasets using Dirichlet distribution, tokenizing the datasets, and creating\n",
    "    DataLoaders for each client's dataset. It also tokenizes, and creates a dataloader for the test dataset.\n",
    "\n",
    "    Args:\n",
    "        ds_path (str): The path or identifier of the dataset.\n",
    "        ds_name (str): The name of the dataset.\n",
    "        checkpoint (str): The checkpoint identifier for the tokenizer.\n",
    "        num_clients (int): The number of clients.\n",
    "        alpha (float): The concentration parameter for the Dirichlet distribution.\n",
    "        batch_size (int): The batch size to be used by the DataLoaders.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - FederatedDataset: An object containing client DataLoaders for federated learning.\n",
    "            - DataLoader: A DataLoader for the tokenized and preprocessed test dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the raw dataset\n",
    "    raw_datasets = load_dataset(path=ds_path, name=ds_name)\n",
    "    \n",
    "    # 1. Test dataset\n",
    "    raw_test_dataset = test_dataset_split(raw_datasets, ds_path)\n",
    "    # 2. Training dataset\n",
    "    raw_train_dataset = raw_datasets['train']\n",
    "\n",
    "    # Calculate the prior distribution\n",
    "    prior_distribution, num_labels = calculate_prior_distribution(raw_train_dataset)\n",
    "\n",
    "    # Partition the dataset into federated datasets\n",
    "    client_datasets = federated_dirichlet_datasets(raw_train_dataset, prior_distribution, num_clients, alpha)\n",
    "\n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    \n",
    "    # Create the tokenization function\n",
    "    tokenize_fn = tokenize_function(ds_path, ds_name, tokenizer)\n",
    "\n",
    "    # Tokenize the client datasets\n",
    "    tok_client_datasets = tokenize_client_datasets(client_datasets, tokenize_fn)\n",
    "\n",
    "    # Create DataLoaders for each client dataset\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    client_dataloaders = create_client_dataloaders(tok_client_datasets, batch_size, data_collator)\n",
    "    \n",
    "    fed_ds = FederatedDataset(client_dataloaders)\n",
    "\n",
    "    # Preprocess the test dataset\n",
    "    test_ds = preprocess_test_dataset(raw_test_dataset, tokenize_fn, data_collator, batch_size)\n",
    "\n",
    "    return fed_ds, test_ds\n",
    "\n",
    "\n",
    "class FederatedDataset:\n",
    "    \"\"\"\n",
    "    A class to handle federated datasets for training in a federated learning setup.\n",
    "\n",
    "    This class encapsulates the logic for managing multiple client dataloaders and providing\n",
    "    batched data for federated learning training loops.\n",
    "\n",
    "    Args:\n",
    "        client_dataloaders (list of DataLoader): A list of DataLoader objects, each corresponding to a client's dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, client_dataloaders):\n",
    "        self.client_dataloaders = client_dataloaders\n",
    "        self.client_batch_generators = [\n",
    "            self.dataloader_batch_generator(client_dl)\n",
    "            for client_dl in client_dataloaders\n",
    "        ]\n",
    "    \n",
    "    def epoch_steps(self, client_ids):\n",
    "        \"\"\"\n",
    "        Determine the number of steps (batches) in an epoch for the given clients.\n",
    "\n",
    "        Args:\n",
    "            client_ids (list of int): A list of client IDs.\n",
    "\n",
    "        Returns:\n",
    "            int: The maximum number of steps (batches) for the given clients.\n",
    "        \"\"\"\n",
    "        \n",
    "        return max(\n",
    "            len(self.client_dataloaders[client_id])\n",
    "            for client_id in client_ids\n",
    "        )\n",
    "    \n",
    "    def next_client_batch(self, client_id):\n",
    "        \"\"\"\n",
    "        Retrieve the next batch of data for the specified client.\n",
    "\n",
    "        Args:\n",
    "            client_id (int): The ID of the client.\n",
    "\n",
    "        Returns:\n",
    "            dict: A batch of data from the client's DataLoader.\n",
    "        \"\"\"\n",
    "        \n",
    "        return next(\n",
    "            self.client_batch_generators[client_id]\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def dataloader_batch_generator(dataloader):\n",
    "        \"\"\"\n",
    "        A generator that yields batches of data from a DataLoader.\n",
    "\n",
    "        Args:\n",
    "            dataloader (DataLoader): A DataLoader object.\n",
    "\n",
    "        Yields:\n",
    "            dict: A batch of data from the DataLoader.\n",
    "        \"\"\"\n",
    "        \n",
    "        while True:\n",
    "            for batch in dataloader:\n",
    "                yield batch\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c62fea1-cdb4-4dfe-91dc-a030b33a9478",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "580f27a12c1f40928e42c01f3b88f778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/37 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parameters\n",
    "ds_path, ds_name = \"glue\", \"mrpc\"\n",
    "num_clients = 100\n",
    "alpha = 1\n",
    "batch_size = 8\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "\n",
    "# Prepare federated datasets and DataLoaders\n",
    "fed_ds, test_ds = prepare_federated_datasets(ds_path, ds_name, checkpoint, num_clients, alpha, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26602a7-c70f-45c7-b600-346fe197b3c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
